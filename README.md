# Stage m2 : higgs events production modes classification



#### Using Monte-Carlo simulated data, train Machine Learning classifiers to reconstruct the production modes of Higgs bosons decaying in the Golden Channel. The performance of the models can be evaluated either directly from their category content plot, or from our custom  "Specificity vs Acceptance" curves (in this implementation, only the metrics on VBF category are considered).



The program uses both ROOT, rootpy and root_numpy, all these should be installed on your system in order to generate the datasets.
We also left the program in a state where it trains our best candidate, the xgboosted trees. Therefore, one should install XGBOOst following the instructions from http://xgboost.readthedocs.io/en/latest/build.html before trying to reproduce the results.


To run the program :
- clone this repository
- move to the production_mode_analysis directory
- run the following command : python main.py --preproc --dispatch --layers &
- watch the logs to make sure the program is running : tail -f logs
- display the figures that were generated in the \saves\figs directory.

Optional arguments (more details in the rest of the text):
--preproc : force run the ROOT preprocessing step
--dispatch : force train the first layer of the model
--layers : force train the second and third layers

Note that by default the program will try to open the intermediate saves, and if this step fails will do the corresponding step of the treatment. The above flags are used to counter this behavior.


## Details:

Most of the program's behavior is controlled from three files: 

- __main.py__ will load all necessary components from the "core" package, and execute them. By acting on this file, one modifies which components of the package will be executed, especially whether or not to compute or recompute certain results. 

- __core\constants.py__ contains most of the parameters controlling the way the functions in the core package are executed. These are meant to be easy to modify without having to change anything in the rest of the package. The most important variables in this module are the list of features, as well as the dictionary of ML models with their associated filename.

- __core\preprocessing.py__ contains the remaining parameters, namely the ones necessary for the preprocessing step. In particular, this is where quantities to retrieve, compute, and remove from the ROOT files for each features set are defined.


the intended behavior is the following :

- __core\preprocessing.py__ takes as an input the full ROOT files for the simulated data in each of the production channels. It returns properly formatted and scaled datasets (scaling is necessary for certain algorithms to perform well) with only the wanted features. It also allows to add calculated features, using either python function or C functions loaded through ROOT.

- __core\trainer.py__ is then called to fit a three-layers model on the training set generated by \textbf{preprocessing}, and generate its predictions. First, a dispatcher is trained to maximize acceptance in each category without considering ggH events, then the two following layers remove first ggH then background events and tune their parameters to maximize the signal strength in each category. All intermediate steps are stored as serialized pickle objects.

- __core\evaluation__ is then used to generate the evaluation metrics for the models previously trained. The save of the predictions made by __trainer__ is very useful to make this step fast as it avoids having to regenerate the predictions, a process which might be long depending on the model we are considering.

